{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9dd9143-d126-4b47-925e-82208d30e294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Evaluating models for scope: ecom\n",
      "‚úÖ Loaded: Logistic Regression.pkl ‚Äî LogisticRegression\n",
      "‚úÖ Loaded: XGBoost.pkl ‚Äî XGBClassifier\n",
      "\n",
      "üîπ Logistic Regression ‚Äî F1: 0.6584, AUC-PR: 0.6458\n",
      "Confusion Matrix:\n",
      " [[26820   573]\n",
      " [ 1160  1670]]\n",
      "\n",
      "üîπ XGBoost ‚Äî F1: 0.5161, AUC-PR: 0.6774\n",
      "Confusion Matrix:\n",
      " [[24916  2477]\n",
      " [  984  1846]]\n",
      "‚úÖ Recommended Model for ECOM: XGBoost\n",
      "üìä Threshold + SHAP plots generated for XGBoost (ecom)\n",
      "\n",
      "üîç Evaluating models for scope: bank\n",
      "‚úÖ Loaded: Logistic Regression.pkl ‚Äî LogisticRegression\n",
      "‚úÖ Loaded: XGBoost.pkl ‚Äî XGBClassifier\n",
      "\n",
      "üîπ Logistic Regression ‚Äî F1: 0.1828, AUC-PR: 0.7311\n",
      "Confusion Matrix:\n",
      " [[55941   710]\n",
      " [   14    81]]\n",
      "\n",
      "üîπ XGBoost ‚Äî F1: 0.8128, AUC-PR: 0.8073\n",
      "Confusion Matrix:\n",
      " [[56635    16]\n",
      " [   19    76]]\n",
      "‚úÖ Recommended Model for BANK: XGBoost\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Imports and Setup\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from src.model_utils import evaluate_model\n",
    "\n",
    "# üìÅ Resolve project root\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else \".\", \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# üì• Model Loading\n",
    "def load_models(prefix):\n",
    "    artifact_dir = os.path.join(project_root, \"artifacts\", prefix)\n",
    "    model_paths = {\n",
    "        \"Logistic Regression\": os.path.join(artifact_dir, \"Logistic Regression.pkl\"),\n",
    "        \"XGBoost\": os.path.join(artifact_dir, \"XGBoost.pkl\")\n",
    "    }\n",
    "    validated_models = {}\n",
    "    for name, path in model_paths.items():\n",
    "        try:\n",
    "            model = joblib.load(path)\n",
    "            if not hasattr(model, \"predict\") or not hasattr(model, \"predict_proba\"):\n",
    "                raise TypeError(f\"{name} is not a valid estimator\")\n",
    "            validated_models[name] = model\n",
    "            print(f\"‚úÖ Loaded: {os.path.basename(path)} ‚Äî {type(model).__name__}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped '{name}': {e}\")\n",
    "    return validated_models\n",
    "\n",
    "# üìä Model Evaluation\n",
    "def evaluate_all(models, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "            result = evaluate_model(y_test, y_pred, y_prob)\n",
    "            if not isinstance(result, dict):\n",
    "                raise ValueError(f\"Invalid result for {name}\")\n",
    "            results[name] = result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Evaluation failed for {name}: {e}\")\n",
    "    return results\n",
    "\n",
    "# üìà Precision-Recall Curve Plot\n",
    "def plot_pr_curve(models, X_test, y_test, prefix):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "            plt.plot(recall, precision, label=name)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped PR curve for {name}: {e}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"{prefix.upper()} ‚Äî Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    plt.savefig(os.path.join(\"models\", f\"{prefix}_pr_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# üìâ Threshold Curve Visualization\n",
    "def plot_threshold_curve(y_true, y_prob, prefix):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(thresholds, precision[:-1], label=\"Precision\")\n",
    "    plt.plot(thresholds, recall[:-1], label=\"Recall\")\n",
    "    plt.plot(thresholds, f1_scores[:-1], label=\"F1 Score\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(f\"{prefix.upper()} ‚Äî Threshold Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(\"models\", f\"{prefix}_threshold_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# üß† SHAP Interpretation\n",
    "def run_shap_analysis(model, X_test, prefix):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
    "    plt.savefig(os.path.join(\"models\", f\"{prefix}_shap_summary_bar.png\"))\n",
    "    plt.close()\n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    plt.savefig(os.path.join(\"models\", f\"{prefix}_shap_summary_beeswarm.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# üìä Confusion Matrix Plot\n",
    "def plot_confusion_matrices(results, prefix):\n",
    "    for name, metrics in results.items():\n",
    "        cm = metrics.get(\"Confusion Matrix\")\n",
    "        if cm is None:\n",
    "            print(f\"‚ö†Ô∏è No Confusion Matrix for {name}\")\n",
    "            continue\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f\"{prefix.upper()} ‚Äî {name} Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        fname = os.path.join(\"models\", f\"{prefix}_{name.lower().replace(' ', '_')}_cm.png\")\n",
    "        plt.savefig(fname)\n",
    "        plt.close()\n",
    "\n",
    "# ‚úÖ Model Recommendation\n",
    "def recommend_model(results, prefix):\n",
    "    try:\n",
    "        best = max(results.items(), key=lambda x: x[1].get(\"AUC-PR\", 0))[0]\n",
    "        print(f\"‚úÖ Recommended Model for {prefix.upper()}: {best}\")\n",
    "        return best\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Recommendation failed for {prefix}: {e}\")\n",
    "        return None\n",
    "\n",
    "# üöÄ Run Evaluation\n",
    "def run_evaluation(prefix):\n",
    "    print(f\"\\nüîç Evaluating models for scope: {prefix}\")\n",
    "    try:\n",
    "        data_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "        X_test = pd.read_csv(os.path.join(data_dir, f\"{prefix}_X_test.csv\"))\n",
    "        y_test = pd.read_csv(os.path.join(data_dir, f\"{prefix}_y_test.csv\"))\n",
    "\n",
    "        models = load_models(prefix)\n",
    "        if not models:\n",
    "            print(f\"üö´ No valid models found for scope='{prefix}'\")\n",
    "            return\n",
    "\n",
    "        results = evaluate_all(models, X_test, y_test)\n",
    "        if not results:\n",
    "            print(f\"‚ö†Ô∏è Evaluation returned no results for scope='{prefix}'\")\n",
    "        else:\n",
    "            for model, metrics in results.items():\n",
    "                print(f\"\\nüîπ {model} ‚Äî F1: {metrics['F1-Score']:.4f}, AUC-PR: {metrics['AUC-PR']:.4f}\")\n",
    "                print(\"Confusion Matrix:\\n\", metrics[\"Confusion Matrix\"])\n",
    "\n",
    "        plot_pr_curve(models, X_test, y_test, prefix)\n",
    "        plot_confusion_matrices(results, prefix)\n",
    "        recommend_model(results, prefix)\n",
    "\n",
    "        # üîç Conditional Explainability for ECOM/XGBoost\n",
    "        if prefix == \"ecom\" and \"XGBoost\" in models:\n",
    "            try:\n",
    "                y_prob = models[\"XGBoost\"].predict_proba(X_test)[:, 1]\n",
    "                plot_threshold_curve(y_test, y_prob, prefix)\n",
    "                run_shap_analysis(models[\"XGBoost\"], X_test, prefix)\n",
    "                print(\"üìä Threshold + SHAP plots generated for XGBoost (ecom)\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Explainability failed for ecom/XGBoost: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üí• Error during evaluation for '{prefix}': {e}\")\n",
    "\n",
    "# üß™ Execute for both scopes\n",
    "if __name__ == \"__main__\":\n",
    "    for scope in [\"ecom\", \"bank\"]:\n",
    "        run_evaluation(scope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ff230-865d-4007-bbb8-02c16282e981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
